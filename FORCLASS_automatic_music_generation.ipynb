{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI Techno Schubert\n",
        "\n",
        "Create Techno Schubert tracks after training on 8 of Schubert's compositions encoded as MIDI files.\n",
        "\n",
        "Start by getting the tracks and a working copy of the model\n",
        "\n",
        "Sources:\n",
        "https://medium.com/@leesurkis/how-to-generate-techno-music-using-deep-learning-17c06910e1b3\n",
        "\n",
        "https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5"
      ],
      "metadata": {
        "id": "A940Ma_0EGih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clean up\n",
        "!rm -Rf AI_schubert ;\n",
        "# clone the repo\n",
        "!git clone https://github.com/douglasgoodwin/AI_schubert ;\n",
        "\n",
        "# clean up the MIDI files\n",
        "!rm -Rf schubert ;\n",
        "!rm -Rf bach ;\n",
        "!rm -Rf techno ;\n",
        "!rm -Rf data ;\n",
        "!rm -Rf weights ;\n",
        "\n",
        "!mv AI_schubert/schubert ./ ;\n",
        "!mv AI_schubert/bach ./ ;\n",
        "!mv AI_schubert/techno ./ ;\n",
        "\n",
        "# move all the models\n",
        "!mv AI_schubert/*.h5 ./ ;\n",
        "\n",
        "# move the weights and data\n",
        "!mv AI_schubert/weights ./ ;\n",
        "!mv AI_schubert/data ./ ;\n",
        "\n",
        "# cleanups\n",
        "!mv *.hdf5 weights/ ;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXciv164lXVs",
        "outputId": "95f64b71-8d54-45b6-8943-36b409ed5bf6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AI_schubert'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects:   9% (1/11)\u001b[K\rremote: Counting objects:  18% (2/11)\u001b[K\rremote: Counting objects:  27% (3/11)\u001b[K\rremote: Counting objects:  36% (4/11)\u001b[K\rremote: Counting objects:  45% (5/11)\u001b[K\rremote: Counting objects:  54% (6/11)\u001b[K\rremote: Counting objects:  63% (7/11)\u001b[K\rremote: Counting objects:  72% (8/11)\u001b[K\rremote: Counting objects:  81% (9/11)\u001b[K\rremote: Counting objects:  90% (10/11)\u001b[K\rremote: Counting objects: 100% (11/11)\u001b[K\rremote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 97 (delta 2), reused 11 (delta 2), pack-reused 86\u001b[K\n",
            "Unpacking objects: 100% (97/97), 90.37 MiB | 6.05 MiB/s, done.\n",
            "mv: cannot stat '*.hdf5': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up some variables\n",
        "# mymodel = 'best_model.h5'\n",
        "# mymodel = 'bach_model.h5'\n",
        "midisongs = \"techno/*.mid\"\n",
        "TRAIN = True"
      ],
      "metadata": {
        "id": "ulcMQQ6NDIYj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHYpVsniMees",
        "outputId": "5a648d6f-c354-4807-a08d-eea3c0b11a3a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install some software"
      ],
      "metadata": {
        "id": "1eaPqaFbEis0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install music21"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEQnzEtTdswq",
        "outputId": "76c5d8f6-e714-4238-a082-b2c90ff99172"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: music21 in /usr/local/lib/python3.9/dist-packages (8.1.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from music21) (1.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from music21) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from music21) (3.7.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.9/dist-packages (from music21) (4.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from music21) (2.27.1)\n",
            "Requirement already satisfied: webcolors>=1.5 in /usr/local/lib/python3.9/dist-packages (from music21) (1.13)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.9/dist-packages (from music21) (9.1.0)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.9/dist-packages (from music21) (3.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->music21) (0.11.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->music21) (5.12.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->music21) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->music21) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->music21) (4.39.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->music21) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->music21) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->music21) (23.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->music21) (1.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->music21) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->music21) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->music21) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->music21) (1.26.15)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->music21) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->music21) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#library for understanding music\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "\n",
        "import glob\n",
        "import pickle\n",
        "import numpy\n",
        "from os.path import exists\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "99dJ5j4odrej"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train a model"
      ],
      "metadata": {
        "id": "9AZbcmXXWhIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code is cleaner that the original\n",
        "# https://github.com/leesurkis/generating-techno-music/blob/master/Generating_Techno_Music-Train.ipynb\n",
        "\n",
        "\n",
        "def train_network():\n",
        "    \"\"\"This function calls all other functions and trains the LSTM\"\"\"\n",
        "    print(\"===== train_network() ======\")\n",
        "\n",
        "    notes = get_notes()\n",
        "\n",
        "    # get amount of pitch names\n",
        "    n_vocab = len(set(notes))\n",
        "    network_input, network_output = prepare_sequences(notes, n_vocab)\n",
        "    model = create_network(network_input, n_vocab)\n",
        "\n",
        "    train(model, network_input, network_output)\n",
        "\n",
        "\n",
        "def get_notes():\n",
        "    \"\"\"Extracts all notes and chords from midi files in the midisongs\n",
        "    directory and creates a file with all notes in string format\"\"\"\n",
        "\n",
        "    print(\"===== get_notes() ======\")\n",
        "    notes = []\n",
        "\n",
        "    for file in glob.glob(midisongs):\n",
        "        midi = converter.parse(file)\n",
        "\n",
        "        print(\"Parsing %s\" % file)\n",
        "\n",
        "        notes_to_parse = None\n",
        "\n",
        "        try:  # file has instrument parts\n",
        "            s2 = instrument.partitionByInstrument(midi)\n",
        "            notes_to_parse = s2.parts[0].recurse()\n",
        "        except:  # file has notes in a flat structure\n",
        "            notes_to_parse = midi.flat.notes\n",
        "\n",
        "        for element in notes_to_parse:\n",
        "            if isinstance(element, note.Note):\n",
        "                notes.append(str(element.pitch))\n",
        "            elif isinstance(element, chord.Chord):\n",
        "                notes.append(\".\".join(str(n) for n in element.normalOrder))\n",
        "\n",
        "    with open(\"data/notes\", \"wb\") as filepath:\n",
        "        pickle.dump(notes, filepath)\n",
        "\n",
        "    return notes\n",
        "\n",
        "\n",
        "def prepare_sequences(notes, n_vocab):\n",
        "    \"\"\"Prepare the sequences which are the inputs for the LSTM\"\"\"\n",
        "\n",
        "    print(\"===== prepare_sequences() ======\")\n",
        "    # sequence length should be changed after experimenting with different numbers\n",
        "    sequence_length = 30\n",
        "\n",
        "    # get all pitch names\n",
        "    pitchnames = sorted(set(item for item in notes))\n",
        "\n",
        "    # create a dictionary to map pitches to integers\n",
        "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    network_input = []\n",
        "    network_output = []\n",
        "\n",
        "    # create input sequences and the corresponding outputs\n",
        "    for i in range(0, len(notes) - sequence_length, 1):\n",
        "        sequence_in = notes[i : i + sequence_length]\n",
        "        sequence_out = notes[i + sequence_length]\n",
        "        network_input.append([note_to_int[char] for char in sequence_in])\n",
        "        network_output.append(note_to_int[sequence_out])\n",
        "\n",
        "    n_patterns = len(network_input)\n",
        "\n",
        "    # reshape the input into a format compatible with LSTM layers\n",
        "    network_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "    # normalize input\n",
        "    network_input = network_input / float(n_vocab)\n",
        "\n",
        "    network_output = np_utils.to_categorical(network_output)\n",
        "\n",
        "    return (network_input, network_output)\n",
        "\n",
        "\n",
        "def create_network(network_input, n_vocab):\n",
        "    \"\"\"Creates the structure of the neural network.\n",
        "    Refactored to work for both training and prediction drg\"\"\"\n",
        "\n",
        "    print(\"===== create_network() ======\")\n",
        "    model = Sequential()\n",
        "    model.add(\n",
        "        LSTM(\n",
        "            512,\n",
        "            input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "            return_sequences=True,\n",
        "        )\n",
        "    )\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(LSTM(512, return_sequences=True))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(LSTM(512))\n",
        "    model.add(Dense(256))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(n_vocab))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
        "\n",
        "    # Load the weights to each node (if weights file is available)\n",
        "    # Now This function should also serve predictions drg\n",
        "    weightsfile = 'weights/weights-improvement-30-0.4634-bigger.hdf5'\n",
        "    # weightsfile = \"/content/weights-improvement-14-3.3951-bigger.hdf5\"\n",
        "    if exists(weightsfile):\n",
        "      model.load_weights(weightsfile)\n",
        "\n",
        "    return model\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "def train(model, network_input, network_output):\n",
        "    \"\"\"train the neural network\"\"\"\n",
        "\n",
        "    print(\"===== train() ======\")\n",
        "    filepath = \"weights/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath, monitor=\"loss\", verbose=0, save_best_only=True, mode=\"min\"\n",
        "    )\n",
        "    callbacks_list = [checkpoint]\n",
        "\n",
        "    # experiment with different epoch sizes and batch sizes\n",
        "    # 15 epochs may be enough\n",
        "    model.fit(\n",
        "        network_input,\n",
        "        network_output,\n",
        "        epochs=15,\n",
        "        batch_size=64,\n",
        "        callbacks=callbacks_list,\n",
        "    )\n",
        "\n",
        "if TRAIN:\n",
        "  print(\"===== TRAINING! ======\")\n",
        "  train_network()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPYtQ09WQc9g",
        "outputId": "f405cd2b-4123-483c-f18d-43d599e74e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/music21/midi/translate.py:863: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent SEQUENCE_TRACK_NAME, track=7, channel=None, data=b'sequenc\\xe9 par '>; getting generic Instrument\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/music21/midi/translate.py:863: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent SEQUENCE_TRACK_NAME, track=8, channel=None, data=b'Jean-Nicholas Ren\\xe9  97'>; getting generic Instrument\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing techno/d-desire.mid\n",
            "Parsing techno/blkisblk_d.mid\n",
            "Parsing techno/d-babybaby.mid\n",
            "Parsing techno/d-bemylover.mid\n",
            "Parsing techno/d-anything.mid\n",
            "Parsing techno/d-becauseimlovingyou.mid\n",
            "Parsing techno/d-dontgoaway.mid\n",
            "Parsing techno/d-badboy.mid\n",
            "Parsing techno/d-allthatshewants.mid\n",
            "Parsing techno/d-children.mid\n",
            "Parsing techno/d-cometakemyhand.mid\n",
            "Parsing techno/d-awayfromhome.mid\n",
            "Parsing techno/d-celebratethelove.mid\n",
            "Parsing techno/d-anotherday.mid\n",
            "Parsing techno/d-closetoyou.mid\n",
            "Parsing techno/d-123trainwithme.mid\n",
            "Parsing techno/d-dontcryformeargentina.mid\n",
            "Parsing techno/d-dadip.mid\n",
            "Parsing techno/d-beautifullife.mid\n",
            "Parsing techno/d-childrenofthenight.mid\n",
            "Parsing techno/d-captainjack.mid\n",
            "Parsing techno/d-100purelove.mid\n",
            "Parsing techno/d-dafunk.mid\n",
            "Parsing techno/blowthe_d.mid\n",
            "Parsing techno/d-comenrideit.mid\n",
            "Parsing techno/d-anothernight.mid\n",
            "Parsing techno/borntobewild2.mid\n",
            "Parsing techno/celebrat_d.mid\n",
            "Parsing techno/d-dancingwithanangel.mid\n",
            "Parsing techno/d-automaticlover.mid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start here if you don't need to train the model."
      ],
      "metadata": {
        "id": "1Yubb8lFWagy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate():\n",
        "    \"\"\"Generates the midi file\"\"\"\n",
        "    print(\"===== generate the MIDI file ======\")\n",
        "    # load the notes used to train the model\n",
        "    with open(\"data/notes\", \"rb\") as filepath:\n",
        "        notes = pickle.load(filepath)\n",
        "\n",
        "    # Get all pitch names\n",
        "    pitchnames = sorted(set(item for item in notes))\n",
        "    # Get all pitch names\n",
        "    n_vocab = len(set(notes))\n",
        "\n",
        "    network_input, normalized_input = prepare_sequences(notes, pitchnames, n_vocab)\n",
        "    model = create_network(normalized_input, n_vocab)\n",
        "    prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)\n",
        "    create_midi(prediction_output)\n",
        "\n",
        "\n",
        "def prepare_sequences(notes, pitchnames, n_vocab):\n",
        "    \"\"\"Prepare the sequences used by the Neural Network\"\"\"\n",
        "\n",
        "    print(\"===== prepare_sequences ======\")\n",
        "    # map back from integers to notes\n",
        "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    sequence_length = 100\n",
        "    network_input = []\n",
        "    output = []\n",
        "    for i in range(0, len(notes) - sequence_length, 1):\n",
        "        sequence_in = notes[i : i + sequence_length]\n",
        "        sequence_out = notes[i + sequence_length]\n",
        "        network_input.append([note_to_int[char] for char in sequence_in])\n",
        "        output.append(note_to_int[sequence_out])\n",
        "\n",
        "    n_patterns = len(network_input)\n",
        "\n",
        "    # reshape the input into a format compatible with LSTM layers\n",
        "    normalized_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "    # normalize input\n",
        "    normalized_input = normalized_input / float(n_vocab)\n",
        "\n",
        "    return (network_input, normalized_input)\n",
        "\n",
        "\n",
        "# def create_network(network_input, n_vocab):\n",
        "#     \"\"\" Creates the structure of the neural network \"\"\"\n",
        "\n",
        "#     model = Sequential()\n",
        "#     model.add(LSTM(\n",
        "#         512,\n",
        "#         input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "#         return_sequences=True\n",
        "#     ))\n",
        "#     model.add(Dropout(0.3))\n",
        "#     model.add(LSTM(512, return_sequences=True))\n",
        "#     model.add(Dropout(0.3))\n",
        "#     model.add(LSTM(512))\n",
        "#     model.add(Dense(256))\n",
        "#     model.add(Dropout(0.3))\n",
        "#     model.add(Dense(n_vocab))\n",
        "#     model.add(Activation('softmax'))\n",
        "#     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "#     # Load the weights to each node\n",
        "#     weightsfile = 'weights/weights-improvement-30-0.4634-bigger.hdf5'\n",
        "#     if exists(weightsfile):\n",
        "#       model.load_weights('weights/weights-improvement-30-0.4634-bigger.hdf5')\n",
        "\n",
        "#     return model\n",
        "\n",
        "\n",
        "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
        "    \"\"\"Generate notes from the neural network based on a sequence of notes\"\"\"\n",
        "\n",
        "    print(\"===== generate_notes ======\")\n",
        "    # pick a random sequence from the input as a starting point for the prediction\n",
        "    start = numpy.random.randint(0, len(network_input) - 1)\n",
        "\n",
        "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    pattern = network_input[start]\n",
        "    prediction_output = []\n",
        "\n",
        "    # generate 500 new notes. See diagram:\n",
        "    # https://miro.medium.com/v2/resize:fit:736/format:webp/1*ACKdHn6zR86lynlr_VEcGw.png\n",
        "\n",
        "    # The code below shows how the model generates 500 notes.\n",
        "    # Create a numpy array of all of the predictions,\n",
        "    # and then index the note with the highest probability.\n",
        "    # This note is then the prediction,\n",
        "    # Append to the input sequence in the next iteration\n",
        "\n",
        "    for note_index in range(500):\n",
        "        prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "        prediction_input = prediction_input / float(n_vocab)\n",
        "\n",
        "        prediction = model.predict(prediction_input, verbose=0)\n",
        "\n",
        "        index = numpy.argmax(prediction)  # numpy array of predictions\n",
        "        result = int_to_note[index]  # indexing the note with the highest probability\n",
        "        prediction_output.append(result)  # that note is the prediction output\n",
        "\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1 : len(pattern)]\n",
        "\n",
        "    return prediction_output\n",
        "\n",
        "\n",
        "def create_midi(prediction_output):\n",
        "\n",
        "    \"\"\"Converts the output from the prediction to notes and create a midi file\n",
        "    from the notes\"\"\"\n",
        "\n",
        "    print(\"===== write the MIDI file ======\")\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for pattern in prediction_output:\n",
        "        # pattern is a chord\n",
        "        if (\".\" in pattern) or pattern.isdigit():\n",
        "            notes_in_chord = pattern.split(\".\")\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                new_note = note.Note(int(current_note))\n",
        "                new_note.storedInstrument = instrument.SnareDrum()\n",
        "                notes.append(new_note)\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "        # pattern is a note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.SnareDrum()\n",
        "            output_notes.append(new_note)\n",
        "\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 0.5\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "    midi_stream.write(\"midi\", fp=\"test_output2.mid\")"
      ],
      "metadata": {
        "id": "OLXqCaF2W6we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate!\n",
        "generate()"
      ],
      "metadata": {
        "id": "KETu1zLIhqDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Ignore this below -- still refactoring"
      ],
      "metadata": {
        "id": "V5lg5m92n80h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Call the pickled notes\n",
        "# with open(\"data/notes\", \"rb\") as filepath:\n",
        "#     notes = pickle.load(filepath)\n",
        "\n",
        "# # prepare the sequences as in the data preparation section\n",
        "# sequence_length = 100\n",
        "# network_input = []\n",
        "# output = []\n",
        "# for i in range(0, len(notes) - sequence_length, 1):\n",
        "#     sequence_in = notes[i : i + sequence_length]\n",
        "#     sequence_out = notes[i + sequence_length]\n",
        "#     network_input.append([note_to_int[char] for char in sequence_in])\n",
        "#     output.append(note_to_int[sequence_out])\n",
        "\n",
        "\n",
        "\n",
        "# prediction_output = []\n",
        "# for note_index in range(500):\n",
        "#     prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "#     prediction_input = prediction_input / float(n_vocab)\n",
        "\n",
        "# prediction = model.predict(prediction_input, verbose=0)\n",
        "# index = numpy.argmax(prediction)\n",
        "\n",
        "# result = int_to_note[index]\n",
        "# prediction_output.append(result)\n",
        "# pattern.append(index)\n",
        "# pattern = pattern[1 : len(pattern)]\n",
        "\n",
        "# # convert the output back into notes and create a midi file.\n",
        "# for pattern in prediction_output:\n",
        "#     if (\".\" in pattern) or pattern.isdigit():\n",
        "#         notes_in_chord = pattern.split(\".\")\n",
        "#         notes = []\n",
        "#         for current_note in notes_in_chord:\n",
        "#             new_note = note.Note(int(current_note))\n",
        "#             new_note.storedInstrument = instrument.SnareDrum()\n",
        "#             notes.append(new_note)\n",
        "#         new_chord = chord.Chord(notes)\n",
        "#         new_chord.offset = offset\n",
        "#         output_notes.append(new_chord)\n",
        "#     else:\n",
        "#         new_note = note.Note(pattern)\n",
        "#         new_note.offset = offset\n",
        "#         new_note.storedInstrument = instrument.SnareDrum()\n",
        "#         output_notes.append(new_note)\n",
        "\n",
        "# offset += 0.5"
      ],
      "metadata": {
        "id": "TgMpoQ7OSy8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AQmCTrvD_-a"
      },
      "outputs": [],
      "source": [
        "# #defining function to read MIDI files\n",
        "# def read_midi(file):\n",
        "    \n",
        "#     print(\"Loading Music File:\",file)\n",
        "    \n",
        "#     notes=[]\n",
        "#     notes_to_parse = None\n",
        "    \n",
        "#     #parsing a midi file\n",
        "#     midi = converter.parse(file)\n",
        "  \n",
        "#     #grouping based on different instruments\n",
        "#     s2 = instrument.partitionByInstrument(midi)\n",
        "\n",
        "#     #Looping over all the instruments\n",
        "#     for part in s2.parts:\n",
        "#       # let's extract all parts\n",
        "#       notes_to_parse = part.recurse() \n",
        "\n",
        "#       #finding whether a particular element is note or a chord\n",
        "#       for element in notes_to_parse:\n",
        "          \n",
        "#           #note\n",
        "#           if isinstance(element, note.Note):\n",
        "#               notes.append(str(element.pitch))\n",
        "          \n",
        "#           #chord\n",
        "#           elif isinstance(element, chord.Chord):\n",
        "#               notes.append('.'.join(str(n) for n in element.normalOrder))\n",
        "\n",
        "#     return np.array(notes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the MIDI files"
      ],
      "metadata": {
        "id": "jm3fzmgZEpER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #for listing down the file names\n",
        "# import os\n",
        "\n",
        "# #Array Processing\n",
        "# import numpy as np\n",
        "\n",
        "# #specify the path\n",
        "# # path='schubert/'\n",
        "# path='/content/bach/'\n",
        "\n",
        "# #read all the filenames\n",
        "# files=[i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
        "\n",
        "# #reading each midi file\n",
        "# notes_array = np.array([read_midi(path+i) for i in files])"
      ],
      "metadata": {
        "id": "sDqin9c4EArZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_notes_chords_rests(path):\n",
        "#     try:\n",
        "#         midi = converter.parse(path)\n",
        "#         parts = instrument.partitionByInstrument(midi)\n",
        "#         note_list = []\n",
        "#         for music_instrument in range(len(parts)):\n",
        "#           for element_by_offset in stream.iterator.OffsetIterator(parts[music_instrument]):\n",
        "#               for entry in element_by_offset:\n",
        "#                   if isinstance(entry, note.Note):\n",
        "#                       note_list.append(str(entry.pitch))\n",
        "#                   elif isinstance(entry, chord.Chord):\n",
        "#                       note_list.append('.'.join(str(n) for n in entry.normalOrder))\n",
        "#                   elif isinstance(entry, note.Rest):\n",
        "#                       note_list.append('Rest')\n",
        "#         return note_list\n",
        "#     except Exception as e:\n",
        "#         print(\"failed on \", path)\n",
        "#         pass\n",
        "\n",
        "# get_notes_chords_rests(\"/content/bach/invent8.mid\")"
      ],
      "metadata": {
        "id": "Ufs7ACJ-Bxzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(notes_array)"
      ],
      "metadata": {
        "id": "sxS957jH_nIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting 2D array into 1D array\n",
        "notes_ = [element for note_ in notes_array for element in note_]\n",
        "\n",
        "#No. of unique notes\n",
        "unique_notes = list(set(notes_))\n",
        "print(len(unique_notes))"
      ],
      "metadata": {
        "id": "OxHXOsuGEKMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick analysis of the notes"
      ],
      "metadata": {
        "id": "3p9PJFR1EvC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing library\n",
        "from collections import Counter\n",
        "\n",
        "#computing frequency of each note\n",
        "freq = dict(Counter(notes_))\n",
        "\n",
        "#library for visualiation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#consider only the frequencies\n",
        "no=[count for _,count in freq.items()]\n",
        "\n",
        "#set the figure size\n",
        "plt.figure(figsize=(5,5))\n",
        "\n",
        "#plot\n",
        "plt.hist(no)"
      ],
      "metadata": {
        "id": "03R3RIKwdHmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
        "print(len(frequent_notes))"
      ],
      "metadata": {
        "id": "RXETMzJXeCRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new_music=[]\n",
        "\n",
        "# for notes in notes_array:\n",
        "#     temp=[]\n",
        "#     for note_ in notes:\n",
        "#         if note_ in frequent_notes:\n",
        "#             temp.append(note_)            \n",
        "#     new_music.append(temp)\n",
        "    \n",
        "# new_music = np.array(new_music)"
      ],
      "metadata": {
        "id": "YAgizSCIdORh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# no_of_timesteps = 32\n",
        "# x = []\n",
        "# y = []\n",
        "\n",
        "# for note_ in new_music:\n",
        "#     for i in range(0, len(note_) - no_of_timesteps, 1):\n",
        "        \n",
        "#         #preparing input and output sequences\n",
        "#         input_ = note_[i:i + no_of_timesteps]\n",
        "#         output = note_[i + no_of_timesteps]\n",
        "        \n",
        "#         x.append(input_)\n",
        "#         y.append(output)\n",
        "        \n",
        "# x=np.array(x)\n",
        "# y=np.array(y)"
      ],
      "metadata": {
        "id": "35Q8wgVPdRuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unique_x = list(set(x.ravel()))\n",
        "# x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))"
      ],
      "metadata": {
        "id": "GHC5nbDMeJqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #preparing input sequences\n",
        "# x_seq=[]\n",
        "# for i in x:\n",
        "#     temp=[]\n",
        "#     for j in i:\n",
        "#         #assigning unique integer to every note\n",
        "#         temp.append(x_note_to_int[j])\n",
        "#     x_seq.append(temp)\n",
        "    \n",
        "# x_seq = np.array(x_seq)"
      ],
      "metadata": {
        "id": "5-XodhM3dUj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def lstm():\n",
        "#   # model = Sequential()\n",
        "#   # model.add(LSTM(128,return_sequences=True))\n",
        "#   # model.add(LSTM(128))\n",
        "#   # model.add(Dense(256))\n",
        "#   # model.add(Activation('relu'))\n",
        "#   # model.add(Dense(n_vocab))\n",
        "#   # model.add(Activation('softmax'))\n",
        "#   # model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "#   # try this:\n",
        "#   # https://medium.com/@leesurkis/how-to-generate-techno-music-using-deep-learning-17c06910e1b3\n",
        "#   model = Sequential()\n",
        "#   model.add(\n",
        "#     LSTM(\n",
        "#       512,\n",
        "#       input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "#       return_sequences=True\n",
        "#     )\n",
        "#   )\n",
        "#   model.add(Dropout(0.3))\n",
        "#   model.add(LSTM(512, return_sequences=True))\n",
        "#   model.add(Dropout(0.3))\n",
        "#   model.add(LSTM(512))\n",
        "#   model.add(Dense(256))\n",
        "#   model.add(Dropout(0.3))\n",
        "#   model.add(Dense(n_vocab))\n",
        "#   model.add(Activation('softmax'))\n",
        "#   model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "#   return model"
      ],
      "metadata": {
        "id": "UoVh1YpTdXYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Similarly, prepare the integer sequences for output data as well\n",
        "\n",
        "# unique_y = list(set(y))\n",
        "# y_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y)) \n",
        "# y_seq=np.array([y_note_to_int[i] for i in y])\n",
        "\n",
        "# # Let us preserve 80% of the data for training and the rest 20% for the evaluation:\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# x_tr, x_val, y_tr, y_val = train_test_split(x_seq,y_seq,test_size=0.2,random_state=0)\n"
      ],
      "metadata": {
        "id": "iTsUSRBHeROw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make the model"
      ],
      "metadata": {
        "id": "53OiK7TEE6LL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras.layers import *\n",
        "# from keras.models import *\n",
        "# from keras.callbacks import *\n",
        "# import keras.backend as K\n",
        "\n",
        "# K.clear_session()\n",
        "# model = Sequential()\n",
        "    \n",
        "# #embedding layer\n",
        "# model.add(Embedding(len(unique_x), 100, input_length=32,trainable=True)) \n",
        "\n",
        "# model.add(Conv1D(64,3, padding='causal',activation='relu'))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(MaxPool1D(2))\n",
        "    \n",
        "# model.add(Conv1D(128,3,activation='relu',dilation_rate=2,padding='causal'))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(MaxPool1D(2))\n",
        "\n",
        "# model.add(Conv1D(256,3,activation='relu',dilation_rate=4,padding='causal'))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(MaxPool1D(2))\n",
        "          \n",
        "# #model.add(Conv1D(256,5,activation='relu'))    \n",
        "# model.add(GlobalMaxPool1D())\n",
        "    \n",
        "# model.add(Dense(256, activation='relu'))\n",
        "# model.add(Dense(len(unique_y), activation='softmax'))\n",
        "    \n",
        "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "FGPWtbbxdZzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the callback to save the best model during training:\n",
        "# mc=ModelCheckpoint(mymodel, monitor='val_loss', mode='min', save_best_only=True,verbose=1)\n",
        "\n",
        "# #Let’s train the model with a batch size of 128 for 50 epochs:\n",
        "# history = model.fit(np.array(x_tr),np.array(y_tr),batch_size=128,epochs=50, validation_data=(np.array(x_val),np.array(y_val)),verbose=1, callbacks=[mc])\n",
        "\n",
        "# # Load the best model:\n",
        "# from keras.models import load_model\n",
        "# model = load_model(mymodel)"
      ],
      "metadata": {
        "id": "vJqxJelJee3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start here if you don't want to improve the model"
      ],
      "metadata": {
        "id": "pvUhevvrrVLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "# ind = np.random.randint(0,len(x_val)-1)\n",
        "\n",
        "# # Load the model (if needed):\n",
        "# if model == None:\n",
        "#   from keras.models import load_model\n",
        "#   model = load_model(mymodel)\n",
        "\n",
        "# random_music = x_val[ind]\n",
        "\n",
        "# predictions=[]\n",
        "# for i in range(100):\n",
        "\n",
        "#     random_music = random_music.reshape(1,no_of_timesteps)\n",
        "\n",
        "#     prob  = model.predict(random_music)[0]\n",
        "#     y_pred= np.argmax(prob,axis=0)\n",
        "#     predictions.append(y_pred)\n",
        "\n",
        "#     random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
        "#     random_music = random_music[1:]\n",
        "    \n",
        "# print(predictions)\n",
        "# # view raw"
      ],
      "metadata": {
        "id": "yCuRDsSedc3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Now, we will convert the integers back into the notes.\n",
        "\n",
        "# x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x)) \n",
        "# predicted_notes = [x_int_to_note[i] for i in predictions]\n",
        "\n",
        "# # The final step is to convert back the predictions into a MIDI file. \n",
        "# # Let’s define the function to accomplish the task."
      ],
      "metadata": {
        "id": "hqmNNzM0eti-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def convert_to_midi(prediction_output):\n",
        "   \n",
        "#     offset = 0\n",
        "#     output_notes = []\n",
        "\n",
        "#     # create note and chord objects based on the values generated by the model\n",
        "#     for pattern in prediction_output:\n",
        "        \n",
        "#         # pattern is a chord\n",
        "#         if ('.' in pattern) or pattern.isdigit():\n",
        "#             notes_in_chord = pattern.split('.')\n",
        "#             notes = []\n",
        "#             for current_note in notes_in_chord:\n",
        "                \n",
        "#                 cn=int(current_note)\n",
        "#                 new_note = note.Note(cn)\n",
        "#                 new_note.storedInstrument = instrument.Piano()\n",
        "#                 notes.append(new_note)\n",
        "                \n",
        "#             new_chord = chord.Chord(notes)\n",
        "#             new_chord.offset = offset\n",
        "#             output_notes.append(new_chord)\n",
        "            \n",
        "#         # pattern is a note\n",
        "#         else:\n",
        "            \n",
        "#             new_note = note.Note(pattern)\n",
        "#             new_note.offset = offset\n",
        "#             new_note.storedInstrument = instrument.Piano()\n",
        "#             output_notes.append(new_note)\n",
        "\n",
        "#         # increase offset each iteration so that notes do not stack\n",
        "#         offset += 1\n",
        "#     midi_stream = stream.Stream(output_notes)\n",
        "#     midi_stream.write('midi', fp='music.mid')"
      ],
      "metadata": {
        "id": "4C0hQU2Wdgj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Convert the predictions into a musical file:\n",
        "# convert_to_midi(predicted_notes)"
      ],
      "metadata": {
        "id": "j_QT6PZtdjgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grab the MIDI file and drop it into a Garageband track"
      ],
      "metadata": {
        "id": "Wb_jMAl6E_cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp music.mid drive/MyDrive/AI/AIbach.mid ;"
      ],
      "metadata": {
        "id": "RN-jS17YE-gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k_MQbWp1MZTQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}